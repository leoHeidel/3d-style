{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import conv_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 256\n",
    "latent_size = 512\n",
    "batch_size = 4\n",
    "cha = 24 #Should be 32\n",
    "n_layers = int(np.log2(im_size) - 1) # 6 for 128 \n",
    "mixed_prob = 0.9\n",
    "channels_mult_list = [1,2,4,6,8,16,32,32,64,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_z(n):\n",
    "    # Z \n",
    "    return np.random.normal(size=[n, latent_size]).astype('float32')\n",
    "\n",
    "def noise_image(batch_size):\n",
    "    return np.random.uniform(size = [batch_size, im_size, im_size, 1]).astype('float32')\n",
    "\n",
    "#Loss functions\n",
    "def gradient_penalty(samples, output, weight):\n",
    "    gradients = K.gradients(output, samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradient_penalty = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "\n",
    "    # (weight / 2) * ||grad||^2\n",
    "    # Penalize the gradient norm\n",
    "    return K.mean(gradient_penalty) * weight\n",
    "\n",
    "def crop_to_fit(x):\n",
    "    height = x[1].shape[1]\n",
    "    width = x[1].shape[2]\n",
    "    return x[0][:, :height, :width, :]\n",
    "\n",
    "def upsample_to_size(x):\n",
    "    y = im_size // x.shape[2]\n",
    "    x = K.resize_images(x, y, y, \"channels_last\",interpolation='bilinear')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_block(x, input_style, input_noise, nb_filters, upsampling = True):\n",
    "    input_filters = x.shape[-1]\n",
    "    if upsampling:\n",
    "        x = keras.layers.UpSampling2D(interpolation='bilinear')(x)\n",
    "    \n",
    "    rgb_style = keras.layers.Dense(nb_filters, kernel_initializer = keras.initializers.VarianceScaling(200/x.shape[2]))(input_style)\n",
    "    style = keras.layers.Dense(input_filters, kernel_initializer = 'he_uniform')(input_style)\n",
    "    \n",
    "    noise_cropped = keras.layers.Lambda(crop_to_fit)([input_noise, x]) ########\n",
    "    d = keras.layers.Dense(nb_filters, kernel_initializer='zeros')(noise_cropped)\n",
    "\n",
    "    x = conv_mod.Conv2DMod(filters=nb_filters, kernel_size = 3, padding = 'same', kernel_initializer = 'he_uniform')([x, style])\n",
    "    x = keras.layers.add([x, d])\n",
    "    x = keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    style = keras.layers.Dense(nb_filters, kernel_initializer = 'he_uniform')(input_style)\n",
    "    d = keras.layers.Dense(nb_filters, kernel_initializer = 'zeros')(noise_cropped)\n",
    "\n",
    "    x = conv_mod.Conv2DMod(filters = nb_filters, kernel_size = 3, padding = 'same', kernel_initializer = 'he_uniform')([x, style])\n",
    "    x = keras.layers.add([x, d])\n",
    "    x = keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    return x, to_rgb(x, rgb_style)\n",
    "\n",
    "def d_block(inp, fil, p = True):\n",
    "    res = keras.layers.Conv2D(fil, 1, kernel_initializer = 'he_uniform')(inp)\n",
    "\n",
    "    out = keras.layers.Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_uniform')(inp)\n",
    "    out = keras.layers.LeakyReLU(0.2)(out)\n",
    "    out = keras.layers.Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_uniform')(out)\n",
    "    out = keras.layers.LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = keras.layers.add([res, out])\n",
    "\n",
    "    if p:\n",
    "        out = keras.layers.AveragePooling2D()(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def to_rgb(inp, style):\n",
    "    size = inp.shape[2]\n",
    "    x = conv_mod.Conv2DMod(3, 1, kernel_initializer = keras.initializers.VarianceScaling(200/size), \n",
    "                              demod = False)([inp, style])\n",
    "    return keras.layers.Lambda(upsample_to_size, output_shape=[None, im_size, im_size, None])(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGan(keras.Model):\n",
    "    def __init__(self, steps = 0, lr = 0.0001, log_steps=None):\n",
    "        super(StyleGan, self).__init__()\n",
    "        \n",
    "        #Models\n",
    "        self.D = self.make_discriminator()\n",
    "        self.S = self.make_style_map()\n",
    "        self.G = self.make_generator()\n",
    "        \n",
    "        #Config\n",
    "        self.LR = lr\n",
    "        self.steps = steps\n",
    "        self.beta = 0.999\n",
    "\n",
    "        self.G_opt = keras.optimizers.Adam(lr = self.LR, beta_1 = 0, beta_2 = 0.999)\n",
    "        self.D_opt = keras.optimizers.Adam(lr = self.LR, beta_1 = 0, beta_2 = 0.999)\n",
    "        \n",
    "        self.pl_mean = tf.Variable(0, dtype=tf.float32)\n",
    "        \n",
    "        logdir = \"logs/train_data/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.log_steps = log_steps\n",
    "        if log_steps is not None:\n",
    "            self.file_writer = tf.summary.create_file_writer(logdir)\n",
    "        \n",
    "    def make_discriminator(self):\n",
    "        d_input = keras.layers.Input(shape = [im_size, im_size, 3])\n",
    "        x = d_input\n",
    "        \n",
    "        nb_d_layers = int(np.log2(im_size))\n",
    "        channels_mult = 1\n",
    "        nb_D_layer = int(np.log2(im_size)) - 1 \n",
    "        for channels_mult in channels_mult_list[:nb_D_layer-1]:\n",
    "            x = d_block(x, channels_mult * cha)\n",
    "        x = d_block(x, channels_mult_list[nb_D_layer-1] * cha, p = False)\n",
    "        \n",
    "        x = keras.layers.Flatten()(x)\n",
    "        x = keras.layers.Dense(1, kernel_initializer = 'he_uniform')(x)\n",
    "        return keras.models.Model(inputs = d_input, outputs = x)\n",
    "\n",
    "    def make_style_map(self):\n",
    "        S = keras.models.Sequential()\n",
    "        S.add(keras.layers.Dense(512, input_shape = [latent_size]))\n",
    "        S.add(keras.layers.LeakyReLU(0.2))\n",
    "        S.add(keras.layers.Dense(512))\n",
    "        S.add(keras.layers.LeakyReLU(0.2))\n",
    "        S.add(keras.layers.Dense(512))\n",
    "        S.add(keras.layers.LeakyReLU(0.2))\n",
    "        S.add(keras.layers.Dense(512))\n",
    "        S.add(keras.layers.LeakyReLU(0.2))\n",
    "        return S\n",
    "    \n",
    "    def make_generator(self):\n",
    "        inp_style = keras.layers.Input([n_layers, 512])\n",
    "        inp_noise = keras.layers.Input([im_size, im_size, 1])\n",
    "    \n",
    "        #Latent\n",
    "        x = inp_style[:,0,:1] * 0 + 1\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        start_dim = im_size // (2**(n_layers-1))\n",
    "\n",
    "        x = keras.layers.Dense(start_dim*start_dim*start_dim*cha, activation = 'relu', \n",
    "                              kernel_initializer = 'random_normal')(x)\n",
    "        x = keras.layers.Reshape([start_dim, start_dim, start_dim*cha])(x)\n",
    "\n",
    "        for i, channels_mult in enumerate(channels_mult_list[:n_layers][::-1]):\n",
    "            x, r = g_block(x, inp_style[:,i], inp_noise, channels_mult * cha, upsampling = (i!=0))  \n",
    "            outs.append(r)\n",
    "        x = keras.layers.add(outs)\n",
    "        x = keras.layers.Lambda(lambda y: y/2 + 0.5)(x) #Use values centered around 0, but normalize to [0, 1], providing better initialization\n",
    "\n",
    "        return keras.models.Model(inputs = [inp_style, inp_noise], outputs = x)\n",
    "\n",
    "    @tf.function\n",
    "    def tf_train_step(self, images, style1, style2, style2_idx, noise, perform_gp=True, perform_pl=False):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            #Get style information\n",
    "            w_1 = self.S(style1)\n",
    "            w_2 = self.S(style2)\n",
    "            w_space = tf.repeat(tf.stack([w_1,w_2], axis=1),[style2_idx, n_layers-style2_idx],axis=1)\n",
    "            pl_lengths = self.pl_mean\n",
    "\n",
    "            #Generate images\n",
    "            generated_images = self.G([w_space,noise])\n",
    "\n",
    "            #Discriminate\n",
    "            real_output = self.D(images, training=True)\n",
    "            fake_output = self.D(generated_images, training=True)\n",
    "\n",
    "            #Hinge loss function\n",
    "            gen_loss = K.mean(fake_output)\n",
    "            divergence = K.mean(K.relu(1 + real_output) + K.relu(1 - fake_output))\n",
    "            disc_loss = divergence\n",
    "\n",
    "            if perform_gp:\n",
    "                #R1 gradient penalty\n",
    "                disc_loss += gradient_penalty(images, real_output, 10)\n",
    "\n",
    "            if perform_pl:\n",
    "                #Slightly adjust W space\n",
    "                w_space_2 = []\n",
    "                for i in range(n_layers):\n",
    "                    w_slice = w_space[:,i]\n",
    "                    std = 0.1 / (K.std(w_slice, axis = 0, keepdims = True) + 1e-8)\n",
    "                    w_space_2.append(w_slice + K.random_normal(tf.shape(w_slice)) / (std + 1e-8))\n",
    "                w_space_2 = tf.stack(w_space_2, axis=1)\n",
    "                #Generate from slightly adjusted W space\n",
    "                pl_images = self.G([w_space_2,noise], training=True)\n",
    "\n",
    "                #Get distance after adjustment (path length)\n",
    "                pl_lengths = K.mean(K.square(pl_images - generated_images), axis = [1, 2, 3])\n",
    "                if self.pl_mean > 0 :\n",
    "                    gen_loss += K.mean(K.square(pl_lengths - self.pl_mean))\n",
    "\n",
    "        #Get gradients for respective areas\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.G.trainable_variables + self.S.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.D.trainable_variables)\n",
    "\n",
    "        #Apply gradients\n",
    "        self.G_opt.apply_gradients(zip(gradients_of_generator, self.G.trainable_variables + self.S.trainable_variables))\n",
    "        self.D_opt.apply_gradients(zip(gradients_of_discriminator, self.D.trainable_variables))\n",
    "\n",
    "        return disc_loss, gen_loss, divergence, pl_lengths\n",
    "    \n",
    "    def train_step(self, args):\n",
    "        images, style1, style2, style2_idx, noise = args\n",
    "        self.steps += 1\n",
    "        \n",
    "        apply_gradient_penalty = self.steps % 2 == 0 or self.steps < 10000\n",
    "        apply_path_penalty = self.steps % 16 == 0\n",
    "        \n",
    "        disc_loss, gen_loss, divergence, pl_lengths = self.tf_train_step(images, style1, style2, \n",
    "                                                                         style2_idx, noise, \n",
    "                                                                         apply_gradient_penalty, \n",
    "                                                                         apply_path_penalty)\n",
    "        \n",
    "        if self.pl_mean == 0:\n",
    "            self.pl_mean.assign(tf.reduce_mean(pl_lengths))\n",
    "        self.pl_mean.assign(0.99*self.pl_mean + 0.01*tf.reduce_mean(pl_lengths))\n",
    "        \n",
    "        if self.log_steps and not self.steps % self.log_steps:\n",
    "            with self.file_writer.as_default():\n",
    "                noise = noise_image(9)\n",
    "                l_z = latent_z(9)\n",
    "                l_w = model.S(l_z)\n",
    "                style = tf.stack([l_w for i in range(n_layers)],axis=1)\n",
    "                generated = model.G([style,noise])\n",
    "                img = tf.concat([tf.concat([generated[3*i+k] for k in range(3)], axis=1) for i in range(3)], axis=0)\n",
    "                tf.summary.image(\"Training data\", [img], step=self.steps)\n",
    "        \n",
    "        return {\n",
    "            \"disc_loss\":disc_loss,\n",
    "            \"gen_loss\":gen_loss,\n",
    "            \"divergence\":divergence,\n",
    "            \"pl_lengths\":pl_lengths,\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(im_path):\n",
    "    im_file = tf.io.read_file(im_path)\n",
    "    im = tf.io.decode_jpeg(im_file, channels=3)\n",
    "    im = tf.image.convert_image_dtype(im, tf.float32)/255\n",
    "    im = tf.image.resize(im, (im_size,im_size))\n",
    "    im = tf.image.random_flip_left_right(im)\n",
    "    return im\n",
    "\n",
    "def train_dataset():\n",
    "    path = \"/Data/leo/dogs-face-2015/*.jpg\"\n",
    "    im_dataset = tf.data.Dataset.list_files(path)\n",
    "    im_dataset = im_dataset.map(read_image)\n",
    "    im_dataset = im_dataset.repeat()\n",
    "    im_dataset = im_dataset.batch(batch_size)\n",
    "    \n",
    "    nb_train_image = len(glob.glob(path))\n",
    "    print(\"Number of train images found:\", nb_train_image)\n",
    "    \n",
    "    def gen_latent_z():\n",
    "        while 1:\n",
    "            yield latent_z(batch_size)\n",
    " \n",
    "    def gen_noise():\n",
    "        while 1:\n",
    "            yield noise_image(batch_size)\n",
    "            \n",
    "    def gen_mixed_idx():\n",
    "        while 1:\n",
    "            if np.random.random() < mixed_prob:\n",
    "                yield np.random.randint(n_layers)\n",
    "            else:\n",
    "                yield n_layers\n",
    "                       \n",
    "    latent_z1_dataset = tf.data.Dataset.from_generator(gen_latent_z, tf.float32, output_shapes=(batch_size, latent_size))\n",
    "    latent_z2_dataset = tf.data.Dataset.from_generator(gen_latent_z, tf.float32, output_shapes=(batch_size, latent_size))\n",
    "    noise_dataset = tf.data.Dataset.from_generator(gen_noise, (tf.float32))\n",
    "    mixed_idx_dataset = tf.data.Dataset.from_generator(gen_mixed_idx, (tf.int32))\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((im_dataset, latent_z1_dataset, latent_z2_dataset, \n",
    "                                   mixed_idx_dataset, noise_dataset))\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StyleGan(log_steps=2000)\n",
    "model.compile(run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eagerly False: 328 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images found: 52597\n",
      "30/30 [==============================] - 18s 611ms/step - disc_loss: 4.9280 - gen_loss: 37.7461 - divergence: 1.9506 - pl_lengths: 0.1298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa90a705460>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = train_dataset().take(30)\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.S.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 24) 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 256, 256, 24) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 24) 5208        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 24) 96          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 256, 256, 24) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256, 256, 24) 0           conv2d[0][0]                     \n",
      "                                                                 leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 128, 128, 24) 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 48) 10416       average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 128, 128, 48) 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 48) 20784       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 48) 1200        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 128, 128, 48) 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 128, 48) 0           conv2d_3[0][0]                   \n",
      "                                                                 leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 64, 64, 48)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 96)   41568       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 64, 64, 96)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 96)   83040       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 96)   4704        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 64, 64, 96)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 96)   0           conv2d_6[0][0]                   \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 32, 32, 96)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 144)  124560      average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 144)  0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 144)  186768      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 144)  13968       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 144)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 144)  0           conv2d_9[0][0]                   \n",
      "                                                                 leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 16, 16, 144)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 192)  249024      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 192)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 192)  331968      leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 192)  27840       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 16, 16, 192)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 192)  0           conv2d_12[0][0]                  \n",
      "                                                                 leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 8, 8, 192)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 384)    663936      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 8, 8, 384)    0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 384)    1327488     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 384)    74112       average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 384)    0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 384)    0           conv2d_15[0][0]                  \n",
      "                                                                 leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 4, 4, 384)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 4, 768)    2654976     average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 4, 4, 768)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 4, 4, 768)    5309184     leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 768)    295680      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 4, 4, 768)    0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 768)    0           conv2d_18[0][0]                  \n",
      "                                                                 leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12288)        0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            12289       flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,439,481\n",
      "Trainable params: 11,439,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(200):\n",
    "#     print(\"Epoch\", i)\n",
    "#     dataset = train_dataset().take(52597//batch_size)\n",
    "#     model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def call(self, l_z, noise):\n",
    "#         l_w = self.S(l_z)\n",
    "#         style = tf.stack([l_w for i in range(n_layers)],axis=1)\n",
    "#         generated = self.G([style,noise])\n",
    "#         return generated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
