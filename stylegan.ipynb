{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import conv_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 64\n",
    "latent_size = 128\n",
    "batch_size = 16\n",
    "cha = 24 #Should be 32\n",
    "n_layers = int(np.log2(im_size) - 2) # 5 for 128 \n",
    "mixed_prob = 0.9\n",
    "channels_mult_list = [1,2,4,6,8,16,32,32,64,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_z(n):\n",
    "    # Z \n",
    "    return np.random.normal(size=[n, latent_size]).astype('float32')\n",
    "\n",
    "def noise_image(batch_size):\n",
    "    return np.random.uniform(size = [batch_size, im_size, im_size, 1]).astype('float32')\n",
    "\n",
    "#Loss functions\n",
    "def gradient_penalty(samples, output, weight):\n",
    "    gradients = K.gradients(output, samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradient_penalty = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "\n",
    "    # (weight / 2) * ||grad||^2\n",
    "    # Penalize the gradient norm\n",
    "    return K.mean(gradient_penalty) * weight\n",
    "\n",
    "def crop_to_fit(x):\n",
    "    height = x[1].shape[1]\n",
    "    width = x[1].shape[2]\n",
    "    return x[0][:, :height, :width, :]\n",
    "\n",
    "def upsample_to_size(x):\n",
    "    y = im_size // x.shape[2]\n",
    "    x = K.resize_images(x, y, y, \"channels_last\",interpolation='bilinear')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_block(x, input_style, input_noise, nb_filters, upsampling = True):\n",
    "    input_filters = x.shape[-1]\n",
    "    if upsampling:\n",
    "        x = keras.layers.UpSampling2D(interpolation='bilinear')(x)\n",
    "    \n",
    "    rgb_style = keras.layers.Dense(nb_filters, kernel_initializer = keras.initializers.VarianceScaling(200/x.shape[2]))(input_style)\n",
    "    style = keras.layers.Dense(input_filters, kernel_initializer = 'he_uniform')(input_style)\n",
    "    \n",
    "    noise_cropped = keras.layers.Lambda(crop_to_fit)([input_noise, x]) ########\n",
    "    d = keras.layers.Dense(nb_filters, kernel_initializer='zeros')(noise_cropped)\n",
    "\n",
    "    x = conv_mod.Conv2DMod(filters=nb_filters, kernel_size = 3, padding = 'same', kernel_initializer = 'he_uniform')([x, style])\n",
    "    x = keras.layers.add([x, d])\n",
    "    x = keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    style = keras.layers.Dense(nb_filters, kernel_initializer = 'he_uniform')(input_style)\n",
    "    d = keras.layers.Dense(nb_filters, kernel_initializer = 'zeros')(noise_cropped)\n",
    "\n",
    "    x = conv_mod.Conv2DMod(filters = nb_filters, kernel_size = 3, padding = 'same', kernel_initializer = 'he_uniform')([x, style])\n",
    "    x = keras.layers.add([x, d])\n",
    "    x = keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    return x, to_rgb(x, rgb_style)\n",
    "\n",
    "def d_block(inp, fil, p = True):\n",
    "    res = keras.layers.Conv2D(fil, 1, kernel_initializer = 'he_uniform')(inp)\n",
    "\n",
    "    out = keras.layers.Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_uniform')(inp)\n",
    "    out = keras.layers.LeakyReLU(0.2)(out)\n",
    "    out = keras.layers.Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_uniform')(out)\n",
    "    out = keras.layers.LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = keras.layers.add([res, out])\n",
    "\n",
    "    if p:\n",
    "        out = keras.layers.AveragePooling2D()(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def to_rgb(inp, style):\n",
    "    size = inp.shape[2]\n",
    "    x = conv_mod.Conv2DMod(3, 1, kernel_initializer = keras.initializers.VarianceScaling(200/size), \n",
    "                              demod = False)([inp, style])\n",
    "    return keras.layers.Lambda(upsample_to_size, output_shape=[None, im_size, im_size, None])(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGan(keras.Model):\n",
    "    def __init__(self, steps = 0, lr = 0.0001, log_steps=None):\n",
    "        super(StyleGan, self).__init__()\n",
    "        \n",
    "        #Models\n",
    "        self.D = self.make_discriminator()\n",
    "        self.S = self.make_style_map()\n",
    "        self.G = self.make_generator()\n",
    "        \n",
    "        #Config\n",
    "        self.LR = lr\n",
    "        self.steps = steps\n",
    "        self.beta = 0.999\n",
    "\n",
    "        self.G_opt = keras.optimizers.Adam(lr = self.LR, beta_1 = 0, beta_2 = 0.999)\n",
    "        self.D_opt = keras.optimizers.Adam(lr = self.LR, beta_1 = 0, beta_2 = 0.999)\n",
    "        \n",
    "        self.pl_mean = tf.Variable(0, dtype=tf.float32)\n",
    "        \n",
    "        logdir = \"logs/train_data/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.log_steps = log_steps\n",
    "        if log_steps is not None:\n",
    "            self.file_writer = tf.summary.create_file_writer(logdir)\n",
    "        \n",
    "    def make_discriminator(self):\n",
    "        d_input = keras.layers.Input(shape = [im_size, im_size, 3])\n",
    "        x = d_input\n",
    "        \n",
    "        nb_d_layers = int(np.log2(im_size))\n",
    "        \n",
    "        channels_mult = 1\n",
    "        nb_D_layer = int(np.log2(im_size))\n",
    "        for channels_mult in channels_mult_list[:nb_D_layer-1]:\n",
    "            x = d_block(x, channels_mult * cha)\n",
    "            channels_mult*=2\n",
    "        x = d_block(x, channels_mult_list[nb_D_layer-1] * cha, p = False)\n",
    "        \n",
    "        x = keras.layers.Flatten()(x)\n",
    "        x = keras.layers.Dense(1, kernel_initializer = 'he_uniform')(x)\n",
    "        return keras.models.Model(inputs = d_input, outputs = x)\n",
    "\n",
    "    def make_style_map(self):\n",
    "        S = keras.models.Sequential()\n",
    "        S.add(keras.layers.Dense(512, input_shape = [latent_size]))\n",
    "        S.add(keras.layers.LeakyReLU(0.2))\n",
    "        S.add(keras.layers.Dense(512))\n",
    "        S.add(keras.layers.LeakyReLU(0.2))\n",
    "        S.add(keras.layers.Dense(512))\n",
    "        S.add(keras.layers.LeakyReLU(0.2))\n",
    "        S.add(keras.layers.Dense(512))\n",
    "        S.add(keras.layers.LeakyReLU(0.2))\n",
    "        return S\n",
    "    \n",
    "    def make_generator(self):\n",
    "        inp_style = keras.layers.Input([n_layers, 512])\n",
    "        inp_noise = keras.layers.Input([im_size, im_size, 1])\n",
    "    \n",
    "        #Latent\n",
    "        x = inp_style[:,0,:1] * 0 + 1\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        start_dim = im_size // (2**n_layers)\n",
    "        x = keras.layers.Dense(start_dim*start_dim*start_dim*cha, activation = 'relu', \n",
    "                              kernel_initializer = 'random_normal')(x)\n",
    "        x = keras.layers.Reshape([start_dim, start_dim, start_dim*cha])(x)\n",
    "\n",
    "        for i, channels_mult in enumerate(channels_mult_list[:n_layers][::-1]):\n",
    "            x, r = g_block(x, inp_style[:,i], inp_noise, channels_mult * cha, upsampling = (i!=0))  \n",
    "            outs.append(r)\n",
    "\n",
    "        x = keras.layers.add(outs)\n",
    "        x = keras.layers.Lambda(lambda y: y/2 + 0.5)(x) #Use values centered around 0, but normalize to [0, 1], providing better initialization\n",
    "\n",
    "        return t\n",
    "\n",
    "    @tf.function\n",
    "    def tf_train_step(self, images, style1, style2, style2_idx, noise, perform_gp=True, perform_pl=False):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            #Get style information\n",
    "            w_1 = self.S(style1)\n",
    "            w_2 = self.S(style2)\n",
    "            w_space = tf.repeat(tf.stack([w_1,w_2], axis=1),[style2_idx, n_layers-style2_idx],axis=1)\n",
    "            pl_lengths = self.pl_mean\n",
    "\n",
    "            #Generate images\n",
    "            generated_images = self.G([w_space,noise])\n",
    "\n",
    "            #Discriminate\n",
    "            real_output = self.D(images, training=True)\n",
    "            fake_output = self.D(generated_images, training=True)\n",
    "\n",
    "            #Hinge loss function\n",
    "            gen_loss = K.mean(fake_output)\n",
    "            divergence = K.mean(K.relu(1 + real_output) + K.relu(1 - fake_output))\n",
    "            disc_loss = divergence\n",
    "\n",
    "            if perform_gp:\n",
    "                #R1 gradient penalty\n",
    "                disc_loss += gradient_penalty(images, real_output, 10)\n",
    "\n",
    "            if perform_pl:\n",
    "                #Slightly adjust W space\n",
    "                w_space_2 = []\n",
    "                for i in range(n_layers):\n",
    "                    w_slice = w_space[:,i]\n",
    "                    std = 0.1 / (K.std(w_slice, axis = 0, keepdims = True) + 1e-8)\n",
    "                    w_space_2.append(w_slice + K.random_normal(tf.shape(w_slice)) / (std + 1e-8))\n",
    "                w_space_2 = tf.stack(w_space_2, axis=1)\n",
    "                #Generate from slightly adjusted W space\n",
    "                pl_images = self.G([w_space_2,noise], training=True)\n",
    "\n",
    "                #Get distance after adjustment (path length)\n",
    "                pl_lengths = K.mean(K.square(pl_images - generated_images), axis = [1, 2, 3])\n",
    "                if self.pl_mean > 0 :\n",
    "                    gen_loss += K.mean(K.square(pl_lengths - self.pl_mean))\n",
    "\n",
    "        #Get gradients for respective areas\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.G.trainable_variables + self.S.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.D.trainable_variables)\n",
    "\n",
    "        #Apply gradients\n",
    "        self.G_opt.apply_gradients(zip(gradients_of_generator, self.G.trainable_variables + self.S.trainable_variables))\n",
    "        self.D_opt.apply_gradients(zip(gradients_of_discriminator, self.D.trainable_variables))\n",
    "\n",
    "        return disc_loss, gen_loss, divergence, pl_lengths\n",
    "    \n",
    "    def train_step(self, args):\n",
    "        images, style1, style2, style2_idx, noise = args\n",
    "        self.steps += 1\n",
    "        \n",
    "        apply_gradient_penalty = self.steps % 2 == 0 or self.steps < 10000\n",
    "        apply_path_penalty = self.steps % 16 == 0\n",
    "        \n",
    "        disc_loss, gen_loss, divergence, pl_lengths = self.tf_train_step(images, style1, style2, \n",
    "                                                                         style2_idx, noise, \n",
    "                                                                         apply_gradient_penalty, \n",
    "                                                                         apply_path_penalty)\n",
    "        \n",
    "        if self.pl_mean == 0:\n",
    "            self.pl_mean.assign(tf.reduce_mean(pl_lengths))\n",
    "        self.pl_mean.assign(0.99*self.pl_mean + 0.01*tf.reduce_mean(pl_lengths))\n",
    "        \n",
    "        if self.log_steps and not self.steps % self.log_steps:\n",
    "            with self.file_writer.as_default():\n",
    "                noise = noise_image(9)\n",
    "                l_z = latent_z(9)\n",
    "                l_w = model.S(l_z)\n",
    "                style = tf.stack([l_w for i in range(n_layers)],axis=1)\n",
    "                generated = model.G([style,noise])\n",
    "                img = tf.concat([tf.concat([generated[3*i+k] for k in range(3)], axis=1) for i in range(3)], axis=0)\n",
    "                tf.summary.image(\"Training data\", [img], step=self.steps)\n",
    "        \n",
    "        return {\n",
    "            \"disc_loss\":disc_loss,\n",
    "            \"gen_loss\":gen_loss,\n",
    "            \"divergence\":divergence,\n",
    "            \"pl_lengths\":pl_lengths,\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(im_path):\n",
    "    im_file = tf.io.read_file(im_path)\n",
    "    im = tf.io.decode_jpeg(im_file, channels=3)\n",
    "    im = tf.image.convert_image_dtype(im, tf.float32)/255\n",
    "    im = tf.image.resize(im, (im_size,im_size))\n",
    "    im = tf.image.random_flip_left_right(im)\n",
    "    return im\n",
    "\n",
    "def train_dataset():\n",
    "    path = \"/Data/leo/dogs-face-2015/*.jpg\"\n",
    "    im_dataset = tf.data.Dataset.list_files(path)\n",
    "    im_dataset = im_dataset.map(read_image)\n",
    "    im_dataset = im_dataset.repeat()\n",
    "    im_dataset = im_dataset.batch(batch_size)\n",
    "    \n",
    "    nb_train_image = len(glob.glob(path))\n",
    "    print(\"Number of train images found:\", nb_train_image)\n",
    "    \n",
    "    def gen_latent_z():\n",
    "        while 1:\n",
    "            yield latent_z(batch_size)\n",
    " \n",
    "    def gen_noise():\n",
    "        while 1:\n",
    "            yield noise_image(batch_size)\n",
    "            \n",
    "    def gen_mixed_idx():\n",
    "        while 1:\n",
    "            if np.random.random() < mixed_prob:\n",
    "                yield np.random.randint(n_layers)\n",
    "            else:\n",
    "                yield n_layers\n",
    "                       \n",
    "    latent_z1_dataset = tf.data.Dataset.from_generator(gen_latent_z, tf.float32, output_shapes=(batch_size, latent_size))\n",
    "    latent_z2_dataset = tf.data.Dataset.from_generator(gen_latent_z, tf.float32, output_shapes=(batch_size, latent_size))\n",
    "    noise_dataset = tf.data.Dataset.from_generator(gen_noise, (tf.float32))\n",
    "    mixed_idx_dataset = tf.data.Dataset.from_generator(gen_mixed_idx, (tf.int32))\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((im_dataset, latent_z1_dataset, latent_z2_dataset, \n",
    "                                   mixed_idx_dataset, noise_dataset))\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StyleGan(log_steps=2000)\n",
    "model.compile(run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eagerly False: 328 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 406s 124ms/step - disc_loss: 1.7376 - gen_loss: 0.5498 - divergence: 1.3579 - pl_lengths: 0.2389\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 402s 122ms/step - disc_loss: 1.7639 - gen_loss: 0.2978 - divergence: 1.5501 - pl_lengths: 0.3381\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 402s 122ms/step - disc_loss: 1.7639 - gen_loss: 0.2938 - divergence: 1.5613 - pl_lengths: 0.3275\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 373s 113ms/step - disc_loss: 1.5684 - gen_loss: 0.4896 - divergence: 1.2748 - pl_lengths: 0.2929\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.4966 - gen_loss: 0.5982 - divergence: 1.1419 - pl_lengths: 0.2659\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.4638 - gen_loss: 0.6494 - divergence: 1.0866 - pl_lengths: 0.2416\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.5002 - gen_loss: 0.6058 - divergence: 1.1625 - pl_lengths: 0.2361\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.4723 - gen_loss: 0.6303 - divergence: 1.1119 - pl_lengths: 0.2507\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 369s 112ms/step - disc_loss: 1.4638 - gen_loss: 0.6511 - divergence: 1.0958 - pl_lengths: 0.2558\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.4307 - gen_loss: 0.6770 - divergence: 1.0411 - pl_lengths: 0.2431\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.3952 - gen_loss: 0.7058 - divergence: 0.9867 - pl_lengths: 0.2323\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.3808 - gen_loss: 0.7257 - divergence: 0.9622 - pl_lengths: 0.2251\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.3655 - gen_loss: 0.7397 - divergence: 0.9392 - pl_lengths: 0.2271\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.3612 - gen_loss: 0.7477 - divergence: 0.9257 - pl_lengths: 0.2173\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.3459 - gen_loss: 0.7587 - divergence: 0.9020 - pl_lengths: 0.2090\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.3302 - gen_loss: 0.7605 - divergence: 0.8839 - pl_lengths: 0.2035\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.3209 - gen_loss: 0.7708 - divergence: 0.8681 - pl_lengths: 0.1991\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.3094 - gen_loss: 0.7743 - divergence: 0.8521 - pl_lengths: 0.1931\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.3052 - gen_loss: 0.7851 - divergence: 0.8415 - pl_lengths: 0.1894\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 369s 112ms/step - disc_loss: 1.2994 - gen_loss: 0.7827 - divergence: 0.8347 - pl_lengths: 0.1890\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 369s 112ms/step - disc_loss: 1.2954 - gen_loss: 0.7892 - divergence: 0.8290 - pl_lengths: 0.1825\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2859 - gen_loss: 0.7924 - divergence: 0.8188 - pl_lengths: 0.1801\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2827 - gen_loss: 0.7998 - divergence: 0.8109 - pl_lengths: 0.1782\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2764 - gen_loss: 0.8013 - divergence: 0.8027 - pl_lengths: 0.1773\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2735 - gen_loss: 0.8018 - divergence: 0.7981 - pl_lengths: 0.1755\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.2662 - gen_loss: 0.8056 - divergence: 0.7893 - pl_lengths: 0.1734\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2627 - gen_loss: 0.8096 - divergence: 0.7823 - pl_lengths: 0.1732\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.2548 - gen_loss: 0.8169 - divergence: 0.7715 - pl_lengths: 0.1687\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2492 - gen_loss: 0.8201 - divergence: 0.7666 - pl_lengths: 0.1658\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2441 - gen_loss: 0.8174 - divergence: 0.7583 - pl_lengths: 0.1639\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2373 - gen_loss: 0.8242 - divergence: 0.7516 - pl_lengths: 0.1645\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2356 - gen_loss: 0.8278 - divergence: 0.7463 - pl_lengths: 0.1614\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2348 - gen_loss: 0.8262 - divergence: 0.7439 - pl_lengths: 0.1586\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2234 - gen_loss: 0.8318 - divergence: 0.7338 - pl_lengths: 0.1574\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2225 - gen_loss: 0.8347 - divergence: 0.7308 - pl_lengths: 0.1538\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2142 - gen_loss: 0.8353 - divergence: 0.7216 - pl_lengths: 0.1545\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.2075 - gen_loss: 0.8391 - divergence: 0.7125 - pl_lengths: 0.1504\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1971 - gen_loss: 0.8415 - divergence: 0.7032 - pl_lengths: 0.1460\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 369s 112ms/step - disc_loss: 1.2027 - gen_loss: 0.8417 - divergence: 0.7087 - pl_lengths: 0.1482\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1886 - gen_loss: 0.8496 - divergence: 0.6950 - pl_lengths: 0.1465\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1894 - gen_loss: 0.8497 - divergence: 0.6934 - pl_lengths: 0.1436\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1870 - gen_loss: 0.8517 - divergence: 0.6877 - pl_lengths: 0.1419\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1802 - gen_loss: 0.8507 - divergence: 0.6824 - pl_lengths: 0.1394\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.1821 - gen_loss: 0.8551 - divergence: 0.6821 - pl_lengths: 0.1393\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1792 - gen_loss: 0.8555 - divergence: 0.6789 - pl_lengths: 0.1413\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1723 - gen_loss: 0.8545 - divergence: 0.6731 - pl_lengths: 0.1430\n",
      "Number of train images found: 52597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1732 - gen_loss: 0.8609 - divergence: 0.6723 - pl_lengths: 0.1404\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1695 - gen_loss: 0.8590 - divergence: 0.6681 - pl_lengths: 0.1399\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.1623 - gen_loss: 0.8622 - divergence: 0.6626 - pl_lengths: 0.1410\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1577 - gen_loss: 0.8602 - divergence: 0.6585 - pl_lengths: 0.1422\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1582 - gen_loss: 0.8655 - divergence: 0.6564 - pl_lengths: 0.1400\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1550 - gen_loss: 0.8647 - divergence: 0.6540 - pl_lengths: 0.1371\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1518 - gen_loss: 0.8656 - divergence: 0.6507 - pl_lengths: 0.1404\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1494 - gen_loss: 0.8691 - divergence: 0.6478 - pl_lengths: 0.1445\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.1466 - gen_loss: 0.8722 - divergence: 0.6442 - pl_lengths: 0.1468\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.1458 - gen_loss: 0.8720 - divergence: 0.6434 - pl_lengths: 0.1383\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 369s 112ms/step - disc_loss: 1.1430 - gen_loss: 0.8767 - divergence: 0.6385 - pl_lengths: 0.1387\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1388 - gen_loss: 0.8761 - divergence: 0.6347 - pl_lengths: 0.1408\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1375 - gen_loss: 0.8772 - divergence: 0.6327 - pl_lengths: 0.1461\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.1390 - gen_loss: 0.8784 - divergence: 0.6322 - pl_lengths: 0.1396\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1316 - gen_loss: 0.8794 - divergence: 0.6266 - pl_lengths: 0.1360\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 113ms/step - disc_loss: 1.1290 - gen_loss: 0.8820 - divergence: 0.6241 - pl_lengths: 0.1395\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 370s 112ms/step - disc_loss: 1.1319 - gen_loss: 0.8779 - divergence: 0.6286 - pl_lengths: 0.1406\n",
      "Number of train images found: 52597\n",
      "3287/3287 [==============================] - 369s 112ms/step - disc_loss: 1.1281 - gen_loss: 0.8809 - divergence: 0.6245 - pl_lengths: 0.1442\n",
      "Number of train images found: 52597\n",
      "   1362/Unknown - 153s 112ms/step - disc_loss: 1.1202 - gen_loss: 0.8831 - divergence: 0.6151 - pl_lengths: 0.1452"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    print(\"Epoch\", i)\n",
    "    dataset = train_dataset().take(52597//batch_size)\n",
    "    model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def call(self, l_z, noise):\n",
    "#         l_w = self.S(l_z)\n",
    "#         style = tf.stack([l_w for i in range(n_layers)],axis=1)\n",
    "#         generated = self.G([style,noise])\n",
    "#         return generated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
